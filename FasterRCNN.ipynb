{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FasterRCNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOQNdhDRTGMir+JDdWEeSZ6"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Faster RCNN"
      ],
      "metadata": {
        "id": "mRrAVOpLJ_pZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Goals:\n",
        "- Training/Inference available to user\n",
        "- Inference:\n",
        "    - ...\n",
        "- Training:\n",
        "    - User submitted parameters:\n",
        "        - k: number of anchors\n",
        "        - pretrained network (ZF/VGG16/user submitted)\n",
        "        - n: spatial window size\n",
        "        - window map output dimension (ZF:256-d/VGG16:512d)\n",
        "        - scale/aspect ratios of anchors\n",
        "        - IoU limits (upper and lower) for flagging anchors\n",
        "        - loss functions: one for Lcls, one for Lreg, one for the combination of the two\n",
        "        - optimizers: one for RPN branch and one for CLS branch\n",
        "        - mini-batch size of randomly sampled anchors (default 256)\n",
        "        - ratio of positive to negative anchors in mini-batch (default 1:1)\n",
        "        - new layer initilization functions (default Gauss 0mean, 0.01std)\n",
        "        - weight initialization for the base (pretrained) model"
      ],
      "metadata": {
        "id": "RJ5EjH4DDIf7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Components\n"
      ],
      "metadata": {
        "id": "Xfzlh6STKBa1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "PZHirgazjl-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data:\n",
        "- PASCAL VOC 2007:\n",
        "    - trained on VOC 2007 trainval\n",
        "    - test on testset"
      ],
      "metadata": {
        "id": "wInYj744kGDB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_datasets as tfds"
      ],
      "metadata": {
        "id": "B9XNzUAIkntv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_original_data_info():\n",
        "    data, info = tfds.load(\"voc\", with_info=True)\n",
        "    test_ds, train_ds, valid_ds = data.values()\n",
        "    return test_ds, train_ds, valid_ds, info\n",
        "test_ds, train_ds, valid_ds, info = get_original_data_info()"
      ],
      "metadata": {
        "id": "y9q3qKrOk4BG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(info)\n",
        "classlbls = info.features['labels']"
      ],
      "metadata": {
        "id": "a4SHDNRcJ8zv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def voc_to_rcnn_bbox(voc_bbox):\n",
        "    # Note: top left is (0,0), bot right is (max,max)\n",
        "    #   - values are normalized b/w [0,1]\n",
        "    #ymin, xmin, ymax, xmax = voc_bbox[:,0], voc_bbox[:,1], voc_bbox[:,2], voc_bbox[:,3]\n",
        "    ymin, xmin, ymax, xmax = tf.split(voc_bbox, num_or_size_splits=4, axis=1)\n",
        "    # Rcnn is in box-center, width, height format\n",
        "    #   - values are still normalized\n",
        "    center_x = (xmin + xmax) / 2.\n",
        "    center_y = (ymin + ymax) / 2.\n",
        "    width = xmax - xmin\n",
        "    height = ymax - ymin\n",
        "    return tf.concat([center_x, center_y, width, height], axis=1)"
      ],
      "metadata": {
        "id": "LD7_uStfMuQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def preprocess_data(sample):\n",
        "    image = sample['image']\n",
        "    bboxes = sample['objects']['bbox']\n",
        "    bboxes = voc_to_rcnn_bbox(bboxes)\n",
        "    labels = sample['objects']['label']\n",
        "    return {'image': image, 'bboxes': bboxes, 'labels': labels}"
      ],
      "metadata": {
        "id": "OtU3WJVUKQEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rcnn_to_pyplot_norm_bbox(rcnn_bbox):\n",
        "    # Note: top left is (0,0), bot right is (max,max)\n",
        "    #   - values a normalized b/w [0,1]\n",
        "    center_x, center_y, width, height = tf.split(rcnn_bbox, num_or_size_splits=4, axis=1)\n",
        "    # Pyplot is in ((xmin, ymin), width, height) format\n",
        "    #   - NOT normalized for plotting, will leave normalized for now\n",
        "    #   - for imshow: top-left is (0,0)\n",
        "    new_xmin = center_x - width/2.\n",
        "    new_ymin = center_y - height/2.\n",
        "    return tf.concat([new_xmin, new_ymin, width, height], axis=1)"
      ],
      "metadata": {
        "id": "2s2f1v8AQUIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# UNTESTED\n",
        "def voc_to_pyplot_norm_bbox(voc_bbox):\n",
        "    # Note: top left is (0,0), bot right is (max,max)\n",
        "    #   - values are normalized b/w [0,1]\n",
        "    ymin, xmin, ymax, xmax = tf.split(voc_bbox, num_or_size_splits=4, axis=1)\n",
        "    # Pyplot is in ((xmin, ymin), width, height) format\n",
        "    #   - NOT normalized for plotting, will leave normalized for now\n",
        "    width = xmax - xmin\n",
        "    height = ymax - ymin\n",
        "    return tf.concat([xmin, ymin, width, height], axis=1)"
      ],
      "metadata": {
        "id": "KyM0cIPePFEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_ds, train_ds, valid_ds, _ = get_original_data_info()\n",
        "test_ds = test_ds.map(preprocess_data)\n",
        "train_ds = train_ds.map(preprocess_data)\n",
        "valid_ds = valid_ds.map(preprocess_data)"
      ],
      "metadata": {
        "id": "wssp9OyNMF7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "def plot_rcnn_bboxes(ax, image, bboxes, labels):\n",
        "    ax.imshow(image)\n",
        "    ax.xaxis.tick_top()\n",
        "    H, W, _ = image.shape\n",
        "    plt_bboxes = rcnn_to_pyplot_norm_bbox(bboxes)\n",
        "    for bbox,label in zip(plt_bboxes, labels):\n",
        "        plt_xmin, plt_ymin, width, height = bbox\n",
        "        rect = patches.Rectangle((plt_xmin*W, plt_ymin*H), width*W, height*H,\n",
        "                                 linewidth=1, edgecolor='r', facecolor='none')\n",
        "        ax.add_patch(rect)\n",
        "        ax.text(plt_xmin*W, plt_ymin*H, classlbls.int2str(label), c='w',\n",
        "                bbox=dict(facecolor='black', edgecolor='black', boxstyle='round'))"
      ],
      "metadata": {
        "id": "e035PX_FX-a2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample = next(iter(test_ds.take(1)))\n",
        "fig, axs = plt.subplots(3,3,figsize=(15,15)); axs = axs.flatten()\n",
        "for ax,sample in zip(axs,test_ds.take(9)):\n",
        "    plot_rcnn_bboxes(ax, **sample)"
      ],
      "metadata": {
        "id": "B8TkiXhxW0Uc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Shared body:\n",
        "- The first network to process the images and produce features that are then fed into the RPN and Classifier head\n",
        "- Pretrained on ImageNet\n",
        "- Archecture of network will determine the receptive field size given the n x n sliding window over the feature layer's outputs"
      ],
      "metadata": {
        "id": "A2hTXQ9NibNt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "VNwjIe2wpXJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained = tf.keras.applications.vgg16.VGG16(\n",
        "    include_top = False, weights='imagenet'\n",
        ")\n",
        "pretrained.trainable = False"
      ],
      "metadata": {
        "id": "dcmblLMSjMip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for sample in train_ds.take(4):\n",
        "    image = sample['image']\n",
        "    print(\"Image shape:\", image.shape)\n",
        "    features = pretrained(image[tf.newaxis, ...])\n",
        "    print(\"Pretrained output shape:\", features.shape)\n",
        "plt.imshow(image)"
      ],
      "metadata": {
        "id": "3fDLgXYxmqWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Approximate method of determining receptive field size:\n",
        "#   - appears that ratio is 3:100 for receptive:base pixels\n",
        "test_image_1 = tf.zeros((1, 100, 100, 3))\n",
        "test_image_2 = tf.zeros((1, 1000, 1000, 3))\n",
        "\n",
        "out_1 = pretrained(test_image_1)\n",
        "out_2 = pretrained(test_image_2)\n",
        "\n",
        "print(out_1.shape)\n",
        "print(out_2.shape)\n",
        "\n",
        "basetoreceptive = 3/100."
      ],
      "metadata": {
        "id": "2BTY_Y4OyvDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RPN"
      ],
      "metadata": {
        "id": "4u-8p5Cdpx66"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RPN(keras.Model):\n",
        "    def __init__(self, n=3, k=9):\n",
        "        super(RPN, self).__init__()\n",
        "\n",
        "        self.k = k\n",
        "        self.n = n\n",
        "\n",
        "        self.windowmap = layers.Conv2D(512, (n,n), activation=\"relu\",\n",
        "                                      name=\"windowmap\")\n",
        "        self.reglayer = layers.Conv2D(4*k, (1,1),\n",
        "                                     name=\"reglayer\")\n",
        "        self.clslayer = layers.Conv2D(2*k, (1,1), activation=\"sigmoid\",\n",
        "                                     name=\"clslayer\")\n",
        "        \n",
        "    def clsloss(self, p, p_tar):\n",
        "        loss = keras.losses.BinaryCrossentropy(p, p_tar)\n",
        "        return loss\n",
        "\n",
        "    def call(self, input_images):\n",
        "        x = self.windowmap(input_images)\n",
        "\n",
        "        reg = self.reglayer(x)\n",
        "        regshape = tf.shape(reg)\n",
        "        reg = tf.reshape(reg, (regshape[1], regshape[2], 9, 4))\n",
        "\n",
        "        cls = self.clslayer(x)\n",
        "        clsshape = tf.shape(cls)\n",
        "        cls = tf.reshape(cls, (clsshape[1], clsshape[2], 9, 2))\n",
        "\n",
        "        return [reg, cls]"
      ],
      "metadata": {
        "id": "7sdJl1W9pxI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rpn = RPN()\n",
        "reg, cls = rpn(features)\n",
        "print(reg.shape)\n",
        "print(cls.shape)"
      ],
      "metadata": {
        "id": "UHrV9amywi6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RoI Pooling"
      ],
      "metadata": {
        "id": "MrMBSg5ZumoA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inputs:\n",
        "#   - RoI BBoxes\n",
        "#   - Features\n",
        "#\n",
        "# Outputs:\n",
        "#   - RoI Feature Vectors\n",
        "\n",
        "\n",
        "class RoIPool(keras.Model):\n",
        "    def __init__(self):\n",
        "        super(RoIPool, self).__init__()\n",
        "\n",
        "        self.flatten = layers.Flatten()\n",
        "        self.fc1 = layers.Dense(32)\n",
        "        self.fc2 = layers.Dense(64)\n",
        "\n",
        "    def pool_roi(self, roi):\n",
        "        # roi :: (None, None, 512)\n",
        "        # pooled_roi :: (7, 7, 512)\n",
        "        roishape = tf.shape(roi)\n",
        "\n",
        "        input = roishape\n",
        "        ksize = [roishape[0] // 7, roishape[1] // 7]\n",
        "        strides = ksize\n",
        "        padding = \"SAME\"\n",
        "\n",
        "        pooled_roi = tf.nn.max_pool2d(input=input,\n",
        "                                      ksize=ksize,\n",
        "                                      strides=strides,\n",
        "                                      padding=padding)\n",
        "        return pooled_roi\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # inputs -- [image_features, reg]\n",
        "        # image_features :: (1, None, None, 512)\n",
        "        # bboxes :: (~2000, 4)\n",
        "        image_features, bbox = inputs\n",
        "        bbox_crop = lambda x: tf.image.crop_to_bounding_box(image_features,\n",
        "                                                            x[0], x[1], x[2], x[3])\n",
        "        rois = tf.map_fn(bbox_crop, bboxes) # :: (~2000, None, None, 512)\n",
        "        pooled_rois = tf.map_fn(self.pool, rois) # :: (~2000, 7, 7, 512)\n",
        "\n",
        "        x = self.flatten(pooled_rois) # :: (~2000, 7 * 7 * 512)\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        return x # :: (~2000, 64)"
      ],
      "metadata": {
        "id": "dRzZoVaLuomx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classifier"
      ],
      "metadata": {
        "id": "R8RIkO6n4T0R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = keras.Input(shape=(64))\n",
        "output = layers.Dense(1000, activation=\"softmax\")(inputs)\n",
        "# Note: could also include an additional bbox regressor\n",
        "#   as is the case in Fast RCNN\n",
        "headclassifier = keras.Model(inputs=inputs, outputs=output)"
      ],
      "metadata": {
        "id": "BmiAe6eeAZOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Full Model"
      ],
      "metadata": {
        "id": "JHiHMXak4V9j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FasterRCNN(keras.Model):\n",
        "    def __init__(self):\n",
        "        super(FasterRCNN, self).__init__()\n",
        "        self.base = pretrained\n",
        "        self.rpn = RPN()\n",
        "        self.roipool = RoIPool()\n",
        "        self.headclassifier = headclassifier\n",
        "\n",
        "    def call(self, input_image):\n",
        "        # input_image :: (1, None, None, 3)\n",
        "\n",
        "        # features :: (1, None, None, 512)\n",
        "        features = self.base(input_image)\n",
        "\n",
        "        # reg :: (None, None, 9, 4)\n",
        "        # cls :: (None, None, 9, 2)\n",
        "        reg, cls = self.rpn(features)\n",
        "\n",
        "        # Perform parameterization ops on reg\n",
        "        # bboxes = f(reg)\n",
        "\n",
        "        # bboxes :: (~2000, 4)\n",
        "        # roifeaturevectors:: (~2000, 64)\n",
        "        roifeaturevectors = self.roipool([features, bboxes])\n",
        "\n",
        "        # class_preds :: (~2000, 1000)\n",
        "        class_preds = self.headclassifier(roifeaturevectors)\n",
        "\n",
        "        return [class_preds, bboxes]"
      ],
      "metadata": {
        "id": "y_RGBlg4BKNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    def __init__(self, anchor_scales=[128, 256, 512], \n",
        "                       anchor_aspectratios=[1., 2., .5]):\n",
        "        super(RoIPool, self).__init__()\n",
        "        self.k = len(anchor_scales) * len(anchor_aspectratios)\n",
        "        self.anchor_scales = tf.floor(tf.constant(anchor_scales) * basetoreceptive)\n",
        "        self.anchor_aspectratios = tf.constant(anchor_aspectratios)"
      ],
      "metadata": {
        "id": "f1lcnVuW5oq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Region Proposal Network (RPN)\n",
        "- input: image of any size\n",
        "- output: set of rectangular object proposals, each w/ an \"objectness\" score\n",
        "- black box: \n",
        "    - n x n spatial window slides across feature map output\n",
        "        - ie. n x n convolutional layer\n",
        "        - w/ n=3, ZF/VGG receptive field size is 171/228 pixels\n",
        "    - window maps to lower-dimensional feature (256-d/512-d w/ ReLU)\n",
        "    - feed feature into 2 sibling FCNs\n",
        "        - Box-regression layer (reg)\n",
        "        - Box-classification layer (cls)\n",
        "        - ie. 1 x 1 convolutional layers\n",
        "    - at each sliding-window location, predict multiple proposals given number of anchors k\n",
        "        - therefore, reg layer has 4k outputs encoding coordinates\n",
        "            - parameterize w.r.t k reference boxes\n",
        "        - therefore, cls layer has 2k outputs estimating probability of object or not object\n",
        "        - each anchor has specified scale and aspect ratio\n",
        "            - use 3 scales, 3 aspect ratios for k = 9 anchors\n",
        "        - translation-invariant anchors:\n",
        "            - regress bounding boxes w/ ref to anchor boxes\n",
        "    - loss function:\n",
        "        - for training, assign binary class label (obj or not) to each anchor\n",
        "            - Positive:\n",
        "                - anchor/anchors w/ highest IoU overlap w/ ground-truth box (rare case)\n",
        "                - OR\n",
        "                - anchor that has IoU overlap higher than 0.7 with any ground-truth box (most common case)\n",
        "            - single ground-truth box may assign positive labels to multiple anchors\n",
        "            - Negative:\n",
        "                - non-positive anchor if its IoU ratio is lower than 0.3 for all ground-truth boxes\n",
        "            - Neither:\n",
        "                - then anchors don't contribute to training\n",
        "        - Loss Function defined in paper\n",
        "            - Lcls is log loss over two classes\n",
        "            - Lreg is robust loss function (ie. smooth L1)\n",
        "            - regression loss activated only for positive anchors\n",
        "            - normalization by scale factor (optional)\n",
        "        - bbox parameterization:\n",
        "            - define w/ log and distance from corners of bbox and size of box\n",
        "            - essentially, regression from anchor box to nearby ground-truth box\n",
        "            - translation-invariant\n",
        "    - Training RPNs\n",
        "        - stochastic gradient descent\n",
        "        - each mini-batch arises from single image containing many positive and negative anchors\n",
        "        - to prevent bias towards the more common negative anchors, randomly sample 256 anchors in image w/ up to 1:1 ratio\n",
        "            - if fewer than 128 positive samples, pad mini-batch w/ negative ones\n",
        "        - randomly initialize all new layers by drawing weights form zero-mean Gauss distrib. w/ std of 0.01\n",
        "            - all other layers (ie. shared conv layers) are initialized by pretraining model for ImageNet classification (ie. fine-tune ZF, VGG, etc.)\n",
        "        - lr = 0.001 for 60k mini-batches\n",
        "        - lr = 0.0001 for next 20k mini-batches\n",
        "        - momentum = 0.9\n",
        "        - weight decay = 0.0005"
      ],
      "metadata": {
        "id": "Bu5YpzeARb05"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training: 4-Step Alternating Training\n",
        "1. fine-tune pre-trained ImageNet model for RPN\n",
        "2. train separate detection network by Fast R-CNN using generated proposals from step-1 (also a pre-trained ImageNet model)\n",
        "3. use detector network to initialize RPN training, but fix the shared conv layers and only fine-tune the layers unique to RPN (now two networks share conv layers)\n",
        "4. keeping shared layers fixed, fine-tune unique layers of Fast R-CNN\n"
      ],
      "metadata": {
        "id": "xB84M5_MaJF-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training: Approximate joint training\n",
        "- RPN and Fast RCNN merged into one network\n",
        "- in each SGD iteration, forward pass generates region proposals which are then passed to the Fast RCNN classifier and are treated as static givens (ie. no backprop path)\n",
        "- backprop is the usual, where shared layers receive backprop signlas from both RPN loss and Fast RCNN loss, combined\n",
        "- Note: soln is easy but ignored the derivative wrt the proposal boxes' coordinates that are also network responses, so is approximate\n",
        "- should produce close results to method 1, but should be faster"
      ],
      "metadata": {
        "id": "cuO7COVYciMr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training: Non-approximate joint training\n",
        "- since the RoI Pooling Layer in FastRCNN accepts the conv features AND the predicted bounding boxes as inputs, grads should theoretically be valid wrt the box coordinates (ie. TF GradientTape should be able to handle the gradient across the bounding box outputs)\n",
        "- would mean we need RoI pooling layer that is differentiable wrt the box coordinates (according to author, nontrivial problem)"
      ],
      "metadata": {
        "id": "lmRTKHKJcp9K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation Details:\n",
        "- re-scale images such that shorter side is s = 600 pixels\n",
        "- for anchors, use 3 scales w/ box areas of 128^2, 256^2, 512^2 and 3 aspect ratios of 1:1, 1:2, 2:1\n",
        "    - note that the bbox prediction is allowed to be larger than the underlying anchor in the receptive field\n",
        "- for anchor boxes that cross image boundaries:\n",
        "    - during training, ignore so they don't contribute to loss\n",
        "        - for 1000 x 600 images, roughly 20000 anchors where 6000 are not cross-boundary\n",
        "    - during testing, clip the proposals that are cross-boundary and return\n",
        "- to reduce redundancy in overlapping proposals, adopt non-max suppression on proposals based on cls scores.\n",
        "    - fix IoU thresh for NMS at 0.7, leaving ultimately about 2000 proposal regions per image\n",
        "    - after NMS, use top-N ranked proposal regions for detection\n",
        "    - Note: can be more lax during training and more strict during testing"
      ],
      "metadata": {
        "id": "HoWK6rIzH44g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiments:\n",
        "- Paper performed experiments on the PASCAL VOC detection benchmark\n",
        "- as well as the MS COCO object detection dataset"
      ],
      "metadata": {
        "id": "zUdpTnZSKCjE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fast R-CNN object detection network"
      ],
      "metadata": {
        "id": "QkaBRcRWRojN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "u-OTcH_z5f7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1VtsHNbdJ-N6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained = tf.keras.applications.vgg16.VGG16(\n",
        "    include_top=False, weights='imagenet', classes=1000\n",
        ")"
      ],
      "metadata": {
        "id": "cJsfxk_86yCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained.summary()"
      ],
      "metadata": {
        "id": "sSXv4D_S7H08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RPN(keras.Model):\n",
        "    def __init__(self, pretrained, n=3, k=9):\n",
        "        super(RPN, self).__init__()\n",
        "        self.pretrained = pretrained\n",
        "\n",
        "        self.windowmap = keras.Conv2D(512, (n,n), activation=\"relu\")\n",
        "        self.reglayer = keras.Conv2D(4*k, (1,1))\n",
        "        self.clslayer = keras.Conv2D(2*k, (1,1), activation=\"sigmoid\")\n",
        "\n",
        "    def call(self, input_images):\n",
        "        x = self.pretrained(input_images)\n",
        "        x = self.windowmap(input_images)\n",
        "        reg = self.reglayer(x)\n",
        "        cls = self.clslayer(x)\n",
        "\n",
        "        "
      ],
      "metadata": {
        "id": "JmXg5_o05nL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Classifier(keras.Model):\n",
        "    def __init__(self, pretrained):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.pretrained = pretrained\n",
        "    \n",
        "    def call(self, input_images):\n",
        "        pass"
      ],
      "metadata": {
        "id": "8zduCAeC968H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FasterRCNN(keras.Model):\n",
        "    def __init__(self, RPN, Classifier):\n",
        "        super(FasterRCNN, self).__init__()\n",
        "        self.RPN = RPN\n",
        "        self.Classifier = Classifier\n",
        "\n",
        "    def compile(self, optimizer):\n",
        "        super(FasterRCNN, self).compile()\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "    def call(self, input_images):\n",
        "        rpn_out = self.RPN(input_images)\n",
        "        classifier_out = self.Classifier(input_images)\n",
        "        return tf.concat([rpn_out, classifier_out])\n",
        "\n",
        "    def train_step(self, input_images):\n",
        "        with tf.GradientTape() as tape:\n",
        "            out = self(input_images)\n",
        "            loss = loss_fn(out, tar)\n",
        "        grads = tape.gradient(loss, self.trainable_weights)\n",
        "        optimizer = self.optimizer.apply_gradients(\n",
        "            zip(grads, self.trainable_weights)\n",
        "        )\n",
        "        return {\"loss\": loss}"
      ],
      "metadata": {
        "id": "VNwzTGq49_Qd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}