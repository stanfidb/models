{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FasterRCNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPEJ7W8BmHTgIzvWEUijehi"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Faster RCNN"
      ],
      "metadata": {
        "id": "mRrAVOpLJ_pZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Goals:\n",
        "- Training/Inference available to user\n",
        "- Inference:\n",
        "    - ...\n",
        "- Training:\n",
        "    - User submitted parameters:\n",
        "        - k: number of anchors\n",
        "        - pretrained network (ZF/VGG16/user submitted)\n",
        "        - n: spatial window size\n",
        "        - window map output dimension (ZF:256-d/VGG16:512d)\n",
        "        - scale/aspect ratios of anchors\n",
        "        - IoU limits (upper and lower) for flagging anchors\n",
        "        - loss functions: one for Lcls, one for Lreg, one for the combination of the two\n",
        "        - optimizers: one for RPN branch and one for CLS branch\n",
        "        - mini-batch size of randomly sampled anchors (default 256)\n",
        "        - ratio of positive to negative anchors in mini-batch (default 1:1)\n",
        "        - new layer initilization functions (default Gauss 0mean, 0.01std)\n",
        "        - weight initialization for the base (pretrained) model"
      ],
      "metadata": {
        "id": "RJ5EjH4DDIf7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Components\n"
      ],
      "metadata": {
        "id": "Xfzlh6STKBa1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "PZHirgazjl-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-addons"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2yCa8-RZaVN",
        "outputId": "a3fd69e8-922f-4836-8e36-3f462dad04c0"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.15.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[?25l\r\u001b[K     |▎                               | 10 kB 3.4 MB/s eta 0:00:01\r\u001b[K     |▋                               | 20 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |▉                               | 30 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 40 kB 9.4 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 51 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 61 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |██                              | 71 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 81 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 92 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |███                             | 102 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 112 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 122 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 133 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 143 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 153 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 163 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████                           | 174 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 184 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 194 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 204 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 215 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 225 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 235 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████                         | 245 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 256 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 266 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████                        | 276 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 286 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 296 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 307 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 317 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 327 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 337 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 348 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 358 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 368 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 378 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 389 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 399 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 409 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 419 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 430 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 440 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 450 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 460 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 471 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 481 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 491 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 501 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 512 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 522 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 532 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 542 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 552 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 563 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 573 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 583 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 593 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 604 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 614 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 624 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 634 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 645 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 655 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 665 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 675 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 686 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 696 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 706 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 716 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 727 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 737 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 747 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 757 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 768 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 778 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 788 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 798 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 808 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 819 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 829 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 839 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 849 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 860 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 870 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 880 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 890 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 901 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 911 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 921 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 931 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 942 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 952 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 962 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 972 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 983 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 993 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.0 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.0 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.0 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.0 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.0 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.1 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.1 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.1 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.1 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.1 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.1 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.1 MB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data:\n",
        "- PASCAL VOC 2007:\n",
        "    - trained on VOC 2007 trainval\n",
        "    - test on testset"
      ],
      "metadata": {
        "id": "wInYj744kGDB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_datasets as tfds"
      ],
      "metadata": {
        "id": "B9XNzUAIkntv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_original_data_info():\n",
        "    data, info = tfds.load(\"voc\", with_info=True)\n",
        "    test_ds, train_ds, valid_ds = data.values()\n",
        "    return test_ds, train_ds, valid_ds, info\n",
        "test_ds, train_ds, valid_ds, info = get_original_data_info()"
      ],
      "metadata": {
        "id": "y9q3qKrOk4BG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(info)\n",
        "classlbls = info.features['labels']"
      ],
      "metadata": {
        "id": "a4SHDNRcJ8zv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def voc_to_rcnn_bbox(voc_bbox):\n",
        "    # Note: top left is (0,0), bot right is (max,max)\n",
        "    #   - values are normalized b/w [0,1]\n",
        "    #ymin, xmin, ymax, xmax = voc_bbox[:,0], voc_bbox[:,1], voc_bbox[:,2], voc_bbox[:,3]\n",
        "    ymin, xmin, ymax, xmax = tf.split(voc_bbox, num_or_size_splits=4, axis=1)\n",
        "    # Rcnn is in box-center, width, height format\n",
        "    #   - values are still normalized\n",
        "    center_x = (xmin + xmax) / 2.\n",
        "    center_y = (ymin + ymax) / 2.\n",
        "    width = xmax - xmin\n",
        "    height = ymax - ymin\n",
        "    return tf.concat([center_x, center_y, width, height], axis=1)"
      ],
      "metadata": {
        "id": "LD7_uStfMuQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rcnn_to_voc_bbox(rcnn_bbox):\n",
        "    # Note: top left is (0,0), bot right is (max,max)\n",
        "    #   - values a normalized b/w [0,1]\n",
        "    center_x, center_y, width, height = tf.split(rcnn_bbox, num_or_size_splits=4, axis=1)\n",
        "    # Pyplot is in ((xmin, ymin), width, height) format\n",
        "    #   - NOT normalized for plotting, will leave normalized for now\n",
        "    #   - for imshow: top-left is (0,0)\n",
        "    xmin = center_x - width/2.\n",
        "    ymin = center_y - height/2.\n",
        "    xmax = center_x + width/2.\n",
        "    ymax = center_y + height/2.\n",
        "    return tf.concat([ymin, xmin, ymax, xmax], axis=1)"
      ],
      "metadata": {
        "id": "UskLorpJJcQq"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def preprocess_data(sample):\n",
        "    image = sample['image']\n",
        "    bboxes = sample['objects']['bbox']\n",
        "    bboxes = voc_to_rcnn_bbox(bboxes)\n",
        "    labels = sample['objects']['label']\n",
        "    return {'image': image, 'bboxes': bboxes, 'labels': labels}"
      ],
      "metadata": {
        "id": "OtU3WJVUKQEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rcnn_to_pyplot_norm_bbox(rcnn_bbox):\n",
        "    # Note: top left is (0,0), bot right is (max,max)\n",
        "    #   - values a normalized b/w [0,1]\n",
        "    center_x, center_y, width, height = tf.split(rcnn_bbox, num_or_size_splits=4, axis=1)\n",
        "    # Pyplot is in ((xmin, ymin), width, height) format\n",
        "    #   - NOT normalized for plotting, will leave normalized for now\n",
        "    #   - for imshow: top-left is (0,0)\n",
        "    new_xmin = center_x - width/2.\n",
        "    new_ymin = center_y - height/2.\n",
        "    return tf.concat([new_xmin, new_ymin, width, height], axis=1)"
      ],
      "metadata": {
        "id": "2s2f1v8AQUIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# UNTESTED\n",
        "def voc_to_pyplot_norm_bbox(voc_bbox):\n",
        "    # Note: top left is (0,0), bot right is (max,max)\n",
        "    #   - values are normalized b/w [0,1]\n",
        "    ymin, xmin, ymax, xmax = tf.split(voc_bbox, num_or_size_splits=4, axis=1)\n",
        "    # Pyplot is in ((xmin, ymin), width, height) format\n",
        "    #   - NOT normalized for plotting, will leave normalized for now\n",
        "    width = xmax - xmin\n",
        "    height = ymax - ymin\n",
        "    return tf.concat([xmin, ymin, width, height], axis=1)"
      ],
      "metadata": {
        "id": "KyM0cIPePFEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_ds, train_ds, valid_ds, _ = get_original_data_info()\n",
        "test_ds = test_ds.map(preprocess_data)\n",
        "train_ds = train_ds.map(preprocess_data)\n",
        "valid_ds = valid_ds.map(preprocess_data)"
      ],
      "metadata": {
        "id": "wssp9OyNMF7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "def plot_rcnn_bboxes(ax, image, bboxes, labels):\n",
        "    ax.imshow(image)\n",
        "    ax.xaxis.tick_top()\n",
        "    H, W, _ = image.shape\n",
        "    plt_bboxes = rcnn_to_pyplot_norm_bbox(bboxes)\n",
        "    for bbox,label in zip(plt_bboxes, labels):\n",
        "        plt_xmin, plt_ymin, width, height = bbox\n",
        "        rect = patches.Rectangle((plt_xmin*W, plt_ymin*H), width*W, height*H,\n",
        "                                 linewidth=1, edgecolor='r', facecolor='none')\n",
        "        ax.add_patch(rect)\n",
        "        ax.text(plt_xmin*W, plt_ymin*H, classlbls.int2str(label), c='w',\n",
        "                bbox=dict(facecolor='black', edgecolor='black', boxstyle='round'))"
      ],
      "metadata": {
        "id": "e035PX_FX-a2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample = next(iter(test_ds.take(1)))\n",
        "fig, axs = plt.subplots(3,3,figsize=(15,15)); axs = axs.flatten()\n",
        "for ax,sample in zip(axs,test_ds.take(9)):\n",
        "    plot_rcnn_bboxes(ax, **sample)"
      ],
      "metadata": {
        "id": "B8TkiXhxW0Uc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Shared body:\n",
        "- The first network to process the images and produce features that are then fed into the RPN and Classifier head\n",
        "- Pretrained on ImageNet\n",
        "- Archecture of network will determine the receptive field size given the n x n sliding window over the feature layer's outputs"
      ],
      "metadata": {
        "id": "A2hTXQ9NibNt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained = tf.keras.applications.vgg16.VGG16(\n",
        "    include_top = False, weights='imagenet'\n",
        ")\n",
        "pretrained.trainable = False # All 13 conv layers are shared "
      ],
      "metadata": {
        "id": "dcmblLMSjMip",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "637bd04f-e391-46fe-91e7-2977345b0fbd"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 1s 0us/step\n",
            "58900480/58889256 [==============================] - 1s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sample in train_ds.take(1):\n",
        "    image = sample['image']\n",
        "    bboxes = sample['bboxes']\n",
        "    labels = sample['labels']\n",
        "    print(\"Image shape:\", image.shape)\n",
        "    features = pretrained(image[tf.newaxis, ...])\n",
        "    print(\"Pretrained output shape:\", features.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fDLgXYxmqWU",
        "outputId": "846a6b43-784a-4dc9-fea7-a89f4bba88f0"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image shape: (480, 389, 3)\n",
            "Pretrained output shape: (1, 15, 12, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine receptive field size:\n",
        "size = 100\n",
        "test = np.ones((size,size))\n",
        "test = np.pad(test, 500-size)\n",
        "test = np.repeat(test[..., np.newaxis], 3, axis=2)\n",
        "\n",
        "fig, ax = plt.subplots(1,2); ax = ax.flatten()\n",
        "ax[0].imshow(test)\n",
        "testout = pretrained(test[np.newaxis, ..., np.newaxis])\n",
        "ax[1].imshow(tf.reduce_sum(testout[0], axis=-1), cmap='gray')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "Q1FW5qePCcCE",
        "outputId": "6dd84125-7ba2-432f-824b-420748099ecc"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f0c90749a50>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Approximate method of determining receptive field size:\n",
        "#   - appears that ratio is 3:100 for receptive:base pixels\n",
        "#   - need better method for determining since paper gives ratio of 3:228\n",
        "test_image_1 = tf.zeros((1, 100, 100, 3))\n",
        "test_image_2 = tf.zeros((1, 1000, 1000, 3))\n",
        "\n",
        "out_1 = pretrained(test_image_1)\n",
        "out_2 = pretrained(test_image_2)\n",
        "\n",
        "print(out_1.shape)\n",
        "print(out_2.shape)\n",
        "\n",
        "#basetoreceptive = 3/100.\n",
        "basetoreceptive = 3/228."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2BTY_Y4OyvDS",
        "outputId": "1222c576-8d35-4927-dd46-0c961f4cb8b3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 3, 3, 512)\n",
            "(1, 31, 31, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RPN\n",
        "- considers provided anchors\n",
        "- proposes based off of IoU limits"
      ],
      "metadata": {
        "id": "4u-8p5Cdpx66"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RPN(keras.Model):\n",
        "    def __init__(self, n=3, k=9):\n",
        "        super(RPN, self).__init__()\n",
        "\n",
        "        self.k = k\n",
        "        self.n = n\n",
        "\n",
        "        self.windowmap = layers.Conv2D(512, (n,n), activation=\"relu\",\n",
        "                                       padding='same', name=\"windowmap\")\n",
        "        self.reglayer = layers.Conv2D(4*k, (1,1),\n",
        "                                      padding='same', name=\"reglayer\")\n",
        "        self.clslayer = layers.Conv2D(2*k, (1,1), activation=\"sigmoid\",\n",
        "                                      padding='same', name=\"clslayer\")\n",
        "        \n",
        "    def clsloss(self, p, p_tar):\n",
        "        loss = keras.losses.BinaryCrossentropy(p, p_tar)\n",
        "        return loss\n",
        "\n",
        "    def call(self, image_features):\n",
        "        x = self.windowmap(image_features)\n",
        "\n",
        "        reg = self.reglayer(x)\n",
        "        regshape = tf.shape(reg)\n",
        "        reg = tf.reshape(reg, (regshape[1], regshape[2], 9, 4))\n",
        "\n",
        "        cls = self.clslayer(x)\n",
        "        clsshape = tf.shape(cls)\n",
        "        cls = tf.reshape(cls, (clsshape[1], clsshape[2], 9, 2))\n",
        "\n",
        "        return [reg, cls]"
      ],
      "metadata": {
        "id": "7sdJl1W9pxI_"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rpn = RPN()\n",
        "reg, cls = rpn(features)\n",
        "print(features.shape)\n",
        "print(reg.shape) # Note: total of W*H*k anchors\n",
        "print(cls.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHrV9amywi6o",
        "outputId": "8e219176-d598-41c9-b3bc-133708dc5166"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 15, 12, 512)\n",
            "(15, 12, 9, 4)\n",
            "(15, 12, 9, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: Loss handling will be done by final class handler\n",
        "\n",
        "# Assign training labels to anchors wrt ground truth:\n",
        "#   - Note: can project an even distribution of anchors according to \n",
        "#           feature layer output dims (ie. H, W)\n",
        "#   - tfa.losses.giou_loss takes format of (xmin, ymin, xmax, ymax) (ie. voc)\n",
        "\n",
        "# For starters, convert rcnn_format to voc_format (ymin, xmin, ymax, xmax)\n",
        "#   - swap to (xmin, ymin, xmax, ymax) format for giou loss function\n",
        "#   - technically, don't need to swap since giou won't tell the difference\n",
        "gtruth_voc_bboxes = rcnn_to_voc_bbox(bboxes)\n",
        "#gtruth_voc_bboxes= tf.stack([voc_bboxes[:,1], voc_bboxes[:,0], voc_bboxes[:,3], voc_bboxes[:,2]], axis=1)\n",
        "\n",
        "# Need to create the static anchor distribution\n",
        "scales = [128., 256., 512.]\n",
        "aspects = [1., .5, 2.]\n",
        "# linspace -> meshgrid\n",
        "img_H, img_W, _ = tf.shape(image)\n",
        "_, feat_H, feat_W, _ = tf.shape(features)\n",
        "# Generate relative sized anchor centers\n",
        "H_linspace = tf.linspace(start=0., stop=1., num=feat_H)\n",
        "W_linspace = tf.linspace(start=0., stop=1., num=feat_W)\n",
        "anchor_centers = tf.stack(tf.meshgrid(W_linspace, H_linspace), axis=-1) # :: (feat_H, feat_W, 2)\n",
        "# Generate relative sized height and width of anchors\n",
        "anchor_W, anchor_Hscales = tf.meshgrid(scales, aspects)\n",
        "anchor_H = anchor_W * anchor_Hscales\n",
        "anchor_W_H = tf.stack([anchor_W, anchor_H], axis=-1)\n",
        "anchor_W_H = tf.reshape(anchor_W_H, [-1, 2]) # :: (9,2)\n",
        "anchor_W_H = anchor_W_H * tf.concat([1./tf.cast(img_W, tf.float32),\n",
        "                                     1./tf.cast(img_H, tf.float32)], axis=0)\n",
        "# Get full relative sized anchor bboxes\n",
        "#   - broadcast for compatible shapes\n",
        "k = 9\n",
        "anchor_centers = tf.broadcast_to(anchor_centers[:, :, tf.newaxis, :], shape=[feat_H, feat_W, k, 2])\n",
        "anchor_W_H = tf.broadcast_to(anchor_W_H, shape=[feat_H, feat_W, k, 2])\n",
        "anchor_rcnn_bboxes = tf.concat([anchor_centers, anchor_W_H], axis=-1) # :: (feat_H, feat_W, 9, 4)\n",
        "\n",
        "# Convert from rcnn format to voc format\n",
        "#   - Note: somme anchors will extend over edge of image\n",
        "# UNVERIFIED\n",
        "anchor_rcnn_bboxes = tf.reshape(anchor_rcnn_bboxes, [-1, 4])\n",
        "anchor_voc_bboxes = rcnn_to_voc_bbox(anchor_rcnn_bboxes)\n",
        "anchor_voc_bboxes = tf.reshape(anchor_rcnn_bboxes, [feat_H, feat_W, k, 4])\n",
        "\n",
        "# Compare all anchors with all bboxes\n",
        "#   - Note: still operating in relative space\n",
        "print(gtruth_voc_bboxes.shape)\n",
        "print(anchor_voc_bboxes.shape)\n",
        "\n",
        "\n",
        "# Anchor labels :: (H, W, 9, 2) -- Positive(1,0), Negative(0,1), Neither(0,0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSEj6ZS3GMRT",
        "outputId": "71810fb5-0ee9-4276-d383-5c1e528ce2e0"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4, 4)\n",
            "(15, 12, 9, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: figure out how to calc. iou then flag the anchors appropriately\n",
        "import tensorflow_addons as tfa\n",
        "test = anchor_voc_bboxes[3,5,0]\n",
        "tfa.losses.giou_loss(gtruth_voc_bboxes, test, mode='iou')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZU8ASjsHYptQ",
        "outputId": "27407646-813f-4a83-ee5b-ed3df44731ce"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(4,), dtype=float32, numpy=array([1., 1., 1., 1.], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RoI Pooling"
      ],
      "metadata": {
        "id": "MrMBSg5ZumoA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inputs:\n",
        "#   - RoI BBoxes\n",
        "#   - Features\n",
        "#\n",
        "# Outputs:\n",
        "#   - RoI Feature Vectors\n",
        "\n",
        "\n",
        "class RoIPool(keras.Model):\n",
        "    def __init__(self):\n",
        "        super(RoIPool, self).__init__()\n",
        "\n",
        "        self.flatten = layers.Flatten()\n",
        "        self.fc1 = layers.Dense(32)\n",
        "        self.fc2 = layers.Dense(64)\n",
        "\n",
        "    def pool_roi(self, roi):\n",
        "        # roi :: (None, None, 512)\n",
        "        # pooled_roi :: (7, 7, 512)\n",
        "        roishape = tf.shape(roi)\n",
        "\n",
        "        input = roishape\n",
        "        ksize = [roishape[0] // 7, roishape[1] // 7]\n",
        "        strides = ksize\n",
        "        padding = \"SAME\"\n",
        "\n",
        "        pooled_roi = tf.nn.max_pool2d(input=input,\n",
        "                                      ksize=ksize,\n",
        "                                      strides=strides,\n",
        "                                      padding=padding)\n",
        "        return pooled_roi\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # inputs -- [image_features, reg]\n",
        "        # image_features :: (1, None, None, 512)\n",
        "        # bboxes :: (~2000, 4)\n",
        "        image_features, bbox = inputs\n",
        "        bbox_crop = lambda x: tf.image.crop_to_bounding_box(image_features,\n",
        "                                                            x[0], x[1], x[2], x[3])\n",
        "        rois = tf.map_fn(bbox_crop, bboxes) # :: (~2000, None, None, 512)\n",
        "        pooled_rois = tf.map_fn(self.pool, rois) # :: (~2000, 7, 7, 512)\n",
        "\n",
        "        x = self.flatten(pooled_rois) # :: (~2000, 7 * 7 * 512)\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        return x # :: (~2000, 64)"
      ],
      "metadata": {
        "id": "dRzZoVaLuomx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classifier"
      ],
      "metadata": {
        "id": "R8RIkO6n4T0R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = keras.Input(shape=(64))\n",
        "output = layers.Dense(1000, activation=\"softmax\")(inputs)\n",
        "# Note: could also include an additional bbox regressor\n",
        "#   as is the case in Fast RCNN\n",
        "headclassifier = keras.Model(inputs=inputs, outputs=output)"
      ],
      "metadata": {
        "id": "BmiAe6eeAZOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Full Model"
      ],
      "metadata": {
        "id": "JHiHMXak4V9j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FasterRCNN(keras.Model):\n",
        "    def __init__(self):\n",
        "        super(FasterRCNN, self).__init__()\n",
        "        self.base = pretrained\n",
        "        self.rpn = RPN()\n",
        "        self.roipool = RoIPool()\n",
        "        self.headclassifier = headclassifier\n",
        "\n",
        "    def call(self, input_image):\n",
        "        # input_image :: (1, None, None, 3)\n",
        "\n",
        "        # features :: (1, None, None, 512)\n",
        "        features = self.base(input_image)\n",
        "\n",
        "        # reg :: (None, None, 9, 4)\n",
        "        # cls :: (None, None, 9, 2)\n",
        "        reg, cls = self.rpn(features)\n",
        "\n",
        "        # Perform parameterization ops on reg\n",
        "        # bboxes = f(reg)\n",
        "\n",
        "        # bboxes :: (~2000, 4)\n",
        "        # roifeaturevectors:: (~2000, 64)\n",
        "        roifeaturevectors = self.roipool([features, bboxes])\n",
        "\n",
        "        # class_preds :: (~2000, 1000)\n",
        "        class_preds = self.headclassifier(roifeaturevectors)\n",
        "\n",
        "        return [class_preds, bboxes]"
      ],
      "metadata": {
        "id": "y_RGBlg4BKNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    def __init__(self, anchor_scales=[128, 256, 512], \n",
        "                       anchor_aspectratios=[1., 2., .5]):\n",
        "        super(RoIPool, self).__init__()\n",
        "        self.k = len(anchor_scales) * len(anchor_aspectratios)\n",
        "        self.anchor_scales = tf.floor(tf.constant(anchor_scales) * basetoreceptive)\n",
        "        self.anchor_aspectratios = tf.constant(anchor_aspectratios)"
      ],
      "metadata": {
        "id": "f1lcnVuW5oq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Region Proposal Network (RPN)\n",
        "- input: image of any size\n",
        "- output: set of rectangular object proposals, each w/ an \"objectness\" score\n",
        "- black box: \n",
        "    - n x n spatial window slides across feature map output\n",
        "        - ie. n x n convolutional layer\n",
        "        - w/ n=3, ZF/VGG receptive field size is 171/228 pixels\n",
        "    - window maps to lower-dimensional feature (256-d/512-d w/ ReLU)\n",
        "    - feed feature into 2 sibling FCNs\n",
        "        - Box-regression layer (reg)\n",
        "        - Box-classification layer (cls)\n",
        "        - ie. 1 x 1 convolutional layers\n",
        "    - at each sliding-window location, predict multiple proposals given number of anchors k\n",
        "        - therefore, reg layer has 4k outputs encoding coordinates\n",
        "            - parameterize w.r.t k reference boxes\n",
        "        - therefore, cls layer has 2k outputs estimating probability of object or not object\n",
        "        - each anchor has specified scale and aspect ratio\n",
        "            - use 3 scales, 3 aspect ratios for k = 9 anchors\n",
        "        - translation-invariant anchors:\n",
        "            - regress bounding boxes w/ ref to anchor boxes\n",
        "    - loss function:\n",
        "        - for training, assign binary class label (obj or not) to each anchor\n",
        "            - Positive:\n",
        "                - anchor/anchors w/ highest IoU overlap w/ ground-truth box (rare case)\n",
        "                - OR\n",
        "                - anchor that has IoU overlap higher than 0.7 with any ground-truth box (most common case)\n",
        "            - single ground-truth box may assign positive labels to multiple anchors\n",
        "            - Negative:\n",
        "                - non-positive anchor if its IoU ratio is lower than 0.3 for all ground-truth boxes\n",
        "            - Neither:\n",
        "                - then anchors don't contribute to training\n",
        "        - Loss Function defined in paper\n",
        "            - Lcls is log loss over two classes\n",
        "            - Lreg is robust loss function (ie. smooth L1)\n",
        "            - regression loss activated only for positive anchors\n",
        "            - normalization by scale factor (optional)\n",
        "        - bbox parameterization:\n",
        "            - define w/ log and distance from corners of bbox and size of box\n",
        "            - essentially, regression from anchor box to nearby ground-truth box\n",
        "            - translation-invariant\n",
        "    - Training RPNs\n",
        "        - stochastic gradient descent\n",
        "        - each mini-batch arises from single image containing many positive and negative anchors\n",
        "        - to prevent bias towards the more common negative anchors, randomly sample 256 anchors in image w/ up to 1:1 ratio\n",
        "            - if fewer than 128 positive samples, pad mini-batch w/ negative ones\n",
        "        - randomly initialize all new layers by drawing weights form zero-mean Gauss distrib. w/ std of 0.01\n",
        "            - all other layers (ie. shared conv layers) are initialized by pretraining model for ImageNet classification (ie. fine-tune ZF, VGG, etc.)\n",
        "        - lr = 0.001 for 60k mini-batches\n",
        "        - lr = 0.0001 for next 20k mini-batches\n",
        "        - momentum = 0.9\n",
        "        - weight decay = 0.0005"
      ],
      "metadata": {
        "id": "Bu5YpzeARb05"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training: 4-Step Alternating Training\n",
        "1. fine-tune pre-trained ImageNet model for RPN\n",
        "2. train separate detection network by Fast R-CNN using generated proposals from step-1 (also a pre-trained ImageNet model)\n",
        "3. use detector network to initialize RPN training, but fix the shared conv layers and only fine-tune the layers unique to RPN (now two networks share conv layers)\n",
        "4. keeping shared layers fixed, fine-tune unique layers of Fast R-CNN\n"
      ],
      "metadata": {
        "id": "xB84M5_MaJF-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training: Approximate joint training\n",
        "- RPN and Fast RCNN merged into one network\n",
        "- in each SGD iteration, forward pass generates region proposals which are then passed to the Fast RCNN classifier and are treated as static givens (ie. no backprop path)\n",
        "- backprop is the usual, where shared layers receive backprop signlas from both RPN loss and Fast RCNN loss, combined\n",
        "- Note: soln is easy but ignored the derivative wrt the proposal boxes' coordinates that are also network responses, so is approximate\n",
        "- should produce close results to method 1, but should be faster"
      ],
      "metadata": {
        "id": "cuO7COVYciMr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training: Non-approximate joint training\n",
        "- since the RoI Pooling Layer in FastRCNN accepts the conv features AND the predicted bounding boxes as inputs, grads should theoretically be valid wrt the box coordinates (ie. TF GradientTape should be able to handle the gradient across the bounding box outputs)\n",
        "- would mean we need RoI pooling layer that is differentiable wrt the box coordinates (according to author, nontrivial problem)"
      ],
      "metadata": {
        "id": "lmRTKHKJcp9K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation Details:\n",
        "- re-scale images such that shorter side is s = 600 pixels\n",
        "- for anchors, use 3 scales w/ box areas of 128^2, 256^2, 512^2 and 3 aspect ratios of 1:1, 1:2, 2:1\n",
        "    - note that the bbox prediction is allowed to be larger than the underlying anchor in the receptive field\n",
        "- for anchor boxes that cross image boundaries:\n",
        "    - during training, ignore so they don't contribute to loss\n",
        "        - for 1000 x 600 images, roughly 20000 anchors where 6000 are not cross-boundary\n",
        "    - during testing, clip the proposals that are cross-boundary and return\n",
        "- to reduce redundancy in overlapping proposals, adopt non-max suppression on proposals based on cls scores.\n",
        "    - fix IoU thresh for NMS at 0.7, leaving ultimately about 2000 proposal regions per image\n",
        "    - after NMS, use top-N ranked proposal regions for detection\n",
        "    - Note: can be more lax during training and more strict during testing"
      ],
      "metadata": {
        "id": "HoWK6rIzH44g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiments:\n",
        "- Paper performed experiments on the PASCAL VOC detection benchmark\n",
        "- as well as the MS COCO object detection dataset"
      ],
      "metadata": {
        "id": "zUdpTnZSKCjE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fast R-CNN object detection network"
      ],
      "metadata": {
        "id": "QkaBRcRWRojN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "u-OTcH_z5f7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1VtsHNbdJ-N6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained = tf.keras.applications.vgg16.VGG16(\n",
        "    include_top=False, weights='imagenet', classes=1000\n",
        ")"
      ],
      "metadata": {
        "id": "cJsfxk_86yCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained.summary()"
      ],
      "metadata": {
        "id": "sSXv4D_S7H08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RPN(keras.Model):\n",
        "    def __init__(self, pretrained, n=3, k=9):\n",
        "        super(RPN, self).__init__()\n",
        "        self.pretrained = pretrained\n",
        "\n",
        "        self.windowmap = keras.Conv2D(512, (n,n), activation=\"relu\")\n",
        "        self.reglayer = keras.Conv2D(4*k, (1,1))\n",
        "        self.clslayer = keras.Conv2D(2*k, (1,1), activation=\"sigmoid\")\n",
        "\n",
        "    def call(self, input_images):\n",
        "        x = self.pretrained(input_images)\n",
        "        x = self.windowmap(input_images)\n",
        "        reg = self.reglayer(x)\n",
        "        cls = self.clslayer(x)\n",
        "\n",
        "        "
      ],
      "metadata": {
        "id": "JmXg5_o05nL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Classifier(keras.Model):\n",
        "    def __init__(self, pretrained):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.pretrained = pretrained\n",
        "    \n",
        "    def call(self, input_images):\n",
        "        pass"
      ],
      "metadata": {
        "id": "8zduCAeC968H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FasterRCNN(keras.Model):\n",
        "    def __init__(self, RPN, Classifier):\n",
        "        super(FasterRCNN, self).__init__()\n",
        "        self.RPN = RPN\n",
        "        self.Classifier = Classifier\n",
        "\n",
        "    def compile(self, optimizer):\n",
        "        super(FasterRCNN, self).compile()\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "    def call(self, input_images):\n",
        "        rpn_out = self.RPN(input_images)\n",
        "        classifier_out = self.Classifier(input_images)\n",
        "        return tf.concat([rpn_out, classifier_out])\n",
        "\n",
        "    def train_step(self, input_images):\n",
        "        with tf.GradientTape() as tape:\n",
        "            out = self(input_images)\n",
        "            loss = loss_fn(out, tar)\n",
        "        grads = tape.gradient(loss, self.trainable_weights)\n",
        "        optimizer = self.optimizer.apply_gradients(\n",
        "            zip(grads, self.trainable_weights)\n",
        "        )\n",
        "        return {\"loss\": loss}"
      ],
      "metadata": {
        "id": "VNwzTGq49_Qd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}