{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FasterRCNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPtm3TbVVfFPEIxBItgeAoR"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Faster RCNN"
      ],
      "metadata": {
        "id": "mRrAVOpLJ_pZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Components\n"
      ],
      "metadata": {
        "id": "Xfzlh6STKBa1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Region Proposal Network (RPN)\n",
        "- input: image of any size\n",
        "- output: set of rectangular object proposals, each w/ an \"objectness\" score\n",
        "- black box: \n",
        "    - n x n spatial window slides across feature map output\n",
        "        - ie. n x n convolutional layer\n",
        "        - w/ n=3, ZF/VGG receptive field size is 171/228 pixels\n",
        "    - window maps to lower-dimensional feature (256-d/512-d w/ ReLU)\n",
        "    - feed feature into 2 sibling FCNs\n",
        "        - Box-regression layer (reg)\n",
        "        - Box-classification layer (cls)\n",
        "        - ie. 1 x 1 convolutional layers\n",
        "    - at each sliding-window location, predict multiple proposals given number of anchors k\n",
        "        - therefore, reg layer has 4k outputs encoding coordinates\n",
        "            - parameterize w.r.t k reference boxes\n",
        "        - therefore, cls layer has 2k outputs estimating probability of object or not object\n",
        "        - each anchor has specified scale and aspect ratio\n",
        "            - use 3 scales, 3 aspect ratios for k = 9 anchors\n",
        "        - translation-invariant anchors:\n",
        "            - regress bounding boxes w/ ref to anchor boxes\n",
        "    - loss function:\n",
        "        - for training, assign binary class label (obj or not) to each anchor\n",
        "            - Positive:\n",
        "                - anchor/anchors w/ highest IoU overlap w/ ground-truth box (rare case)\n",
        "                - OR\n",
        "                - anchor that has IoU overlap higher than 0.7 with any ground-truth box (most common case)\n",
        "            - single ground-truth box may assign positive labels to multiple anchors\n",
        "            - Negative:\n",
        "                - non-positive anchor if its IoU ratio is lower than 0.3 for all ground-truth boxes\n",
        "            - Neither:\n",
        "                - then anchors don't contribute to training\n",
        "        - Loss Function defined in paper\n",
        "            - Lcls is log loss over two classes\n",
        "            - Lreg is robust loss function (ie. smooth L1)\n",
        "            - regression loss activated only for positive anchors\n",
        "            - normalization by scale factor (optional)\n",
        "        - bbox parameterization:\n",
        "            - define w/ log and distance from corners of bbox and size of box\n",
        "            - essentially, regression from anchor box to nearby ground-truth box\n",
        "            - translation-invariant\n",
        "    - Training RPNs\n",
        "        - stochastic gradient descent\n",
        "        - each mini-batch arises from single image containing many positive and negative anchors\n",
        "        - to prevent bias towards the more common negative anchors, randomly sample 256 anchors in image w/ up to 1:1 ratio\n",
        "            - if fewer than 128 positive samples, pad mini-batch w/ negative ones\n",
        "        - randomly initialize all new layers by drawing weights form zero-mean Gauss distrib. w/ std of 0.01\n",
        "            - all other layers (ie. shared conv layers) are initialized by pretraining model for ImageNet classification (ie. fine-tune ZF, VGG, etc.)\n",
        "        - lr = 0.001 for 60k mini-batches\n",
        "        - lr = 0.0001 for next 20k mini-batches\n",
        "        - momentum = 0.9\n",
        "        - weight decay = 0.0005"
      ],
      "metadata": {
        "id": "Bu5YpzeARb05"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training: 4-Step Alternating Training\n",
        "1. fine-tune pre-trained ImageNet model for RPN\n",
        "2. train separate detection network by Fast R-CNN using generated proposals from step-1 (also a pre-trained ImageNet model)\n",
        "3. use detector network to initialize RPN training, but fix the shared conv layers and only fine-tune the layers unique to RPN (now two networks share conv layers)\n",
        "4. keeping shared layers fixed, fine-tune unique layers of Fast R-CNN\n"
      ],
      "metadata": {
        "id": "xB84M5_MaJF-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training: Approximate joint training"
      ],
      "metadata": {
        "id": "cuO7COVYciMr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training: Non-approximate joint training"
      ],
      "metadata": {
        "id": "lmRTKHKJcp9K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fast R-CNN object detection network"
      ],
      "metadata": {
        "id": "QkaBRcRWRojN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1VtsHNbdJ-N6"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ]
}