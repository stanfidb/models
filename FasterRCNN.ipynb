{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FasterRCNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM7R0JBnv6zpVtiz9Dl8zXf"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Faster RCNN"
      ],
      "metadata": {
        "id": "mRrAVOpLJ_pZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Goals:\n",
        "- Training/Inference available to user\n",
        "- Inference:\n",
        "    - ...\n",
        "- Training:\n",
        "    - User submitted parameters:\n",
        "        - k: number of anchors\n",
        "        - pretrained network (ZF/VGG16/user submitted)\n",
        "        - n: spatial window size\n",
        "        - window map output dimension (ZF:256-d/VGG16:512d)\n",
        "        - scale/aspect ratios of anchors\n",
        "        - IoU limits (upper and lower) for flagging anchors\n",
        "        - loss functions: one for Lcls, one for Lreg, one for the combination of the two\n",
        "        - optimizers: one for RPN branch and one for CLS branch\n",
        "        - mini-batch size of randomly sampled anchors (default 256)\n",
        "        - ratio of positive to negative anchors in mini-batch (default 1:1)\n",
        "        - new layer initilization functions (default Gauss 0mean, 0.01std)\n",
        "        - weight initialization for the base (pretrained) model"
      ],
      "metadata": {
        "id": "RJ5EjH4DDIf7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Components\n"
      ],
      "metadata": {
        "id": "Xfzlh6STKBa1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Region Proposal Network (RPN)\n",
        "- input: image of any size\n",
        "- output: set of rectangular object proposals, each w/ an \"objectness\" score\n",
        "- black box: \n",
        "    - n x n spatial window slides across feature map output\n",
        "        - ie. n x n convolutional layer\n",
        "        - w/ n=3, ZF/VGG receptive field size is 171/228 pixels\n",
        "    - window maps to lower-dimensional feature (256-d/512-d w/ ReLU)\n",
        "    - feed feature into 2 sibling FCNs\n",
        "        - Box-regression layer (reg)\n",
        "        - Box-classification layer (cls)\n",
        "        - ie. 1 x 1 convolutional layers\n",
        "    - at each sliding-window location, predict multiple proposals given number of anchors k\n",
        "        - therefore, reg layer has 4k outputs encoding coordinates\n",
        "            - parameterize w.r.t k reference boxes\n",
        "        - therefore, cls layer has 2k outputs estimating probability of object or not object\n",
        "        - each anchor has specified scale and aspect ratio\n",
        "            - use 3 scales, 3 aspect ratios for k = 9 anchors\n",
        "        - translation-invariant anchors:\n",
        "            - regress bounding boxes w/ ref to anchor boxes\n",
        "    - loss function:\n",
        "        - for training, assign binary class label (obj or not) to each anchor\n",
        "            - Positive:\n",
        "                - anchor/anchors w/ highest IoU overlap w/ ground-truth box (rare case)\n",
        "                - OR\n",
        "                - anchor that has IoU overlap higher than 0.7 with any ground-truth box (most common case)\n",
        "            - single ground-truth box may assign positive labels to multiple anchors\n",
        "            - Negative:\n",
        "                - non-positive anchor if its IoU ratio is lower than 0.3 for all ground-truth boxes\n",
        "            - Neither:\n",
        "                - then anchors don't contribute to training\n",
        "        - Loss Function defined in paper\n",
        "            - Lcls is log loss over two classes\n",
        "            - Lreg is robust loss function (ie. smooth L1)\n",
        "            - regression loss activated only for positive anchors\n",
        "            - normalization by scale factor (optional)\n",
        "        - bbox parameterization:\n",
        "            - define w/ log and distance from corners of bbox and size of box\n",
        "            - essentially, regression from anchor box to nearby ground-truth box\n",
        "            - translation-invariant\n",
        "    - Training RPNs\n",
        "        - stochastic gradient descent\n",
        "        - each mini-batch arises from single image containing many positive and negative anchors\n",
        "        - to prevent bias towards the more common negative anchors, randomly sample 256 anchors in image w/ up to 1:1 ratio\n",
        "            - if fewer than 128 positive samples, pad mini-batch w/ negative ones\n",
        "        - randomly initialize all new layers by drawing weights form zero-mean Gauss distrib. w/ std of 0.01\n",
        "            - all other layers (ie. shared conv layers) are initialized by pretraining model for ImageNet classification (ie. fine-tune ZF, VGG, etc.)\n",
        "        - lr = 0.001 for 60k mini-batches\n",
        "        - lr = 0.0001 for next 20k mini-batches\n",
        "        - momentum = 0.9\n",
        "        - weight decay = 0.0005"
      ],
      "metadata": {
        "id": "Bu5YpzeARb05"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training: 4-Step Alternating Training\n",
        "1. fine-tune pre-trained ImageNet model for RPN\n",
        "2. train separate detection network by Fast R-CNN using generated proposals from step-1 (also a pre-trained ImageNet model)\n",
        "3. use detector network to initialize RPN training, but fix the shared conv layers and only fine-tune the layers unique to RPN (now two networks share conv layers)\n",
        "4. keeping shared layers fixed, fine-tune unique layers of Fast R-CNN\n"
      ],
      "metadata": {
        "id": "xB84M5_MaJF-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training: Approximate joint training\n",
        "- RPN and Fast RCNN merged into one network\n",
        "- in each SGD iteration, forward pass generates region proposals which are then passed to the Fast RCNN classifier and are treated as static givens (ie. no backprop path)\n",
        "- backprop is the usual, where shared layers receive backprop signlas from both RPN loss and Fast RCNN loss, combined\n",
        "- Note: soln is easy but ignored the derivative wrt the proposal boxes' coordinates that are also network responses, so is approximate\n",
        "- should produce close results to method 1, but should be faster"
      ],
      "metadata": {
        "id": "cuO7COVYciMr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training: Non-approximate joint training\n",
        "- since the RoI Pooling Layer in FastRCNN accepts the conv features AND the predicted bounding boxes as inputs, grads should theoretically be valid wrt the box coordinates (ie. TF GradientTape should be able to handle the gradient across the bounding box outputs)\n",
        "- would mean we need RoI pooling layer that is differentiable wrt the box coordinates (according to author, nontrivial problem)"
      ],
      "metadata": {
        "id": "lmRTKHKJcp9K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation Details:\n",
        "- re-scale images such that shorter side is s = 600 pixels\n",
        "- for anchors, use 3 scales w/ box areas of 128^2, 256^2, 512^2 and 3 aspect ratios of 1:1, 1:2, 2:1\n",
        "    - note that the bbox prediction is allowed to be larger than the underlying anchor in the receptive field\n",
        "- for anchor boxes that cross image boundaries:\n",
        "    - during training, ignore so they don't contribute to loss\n",
        "        - for 1000 x 600 images, roughly 20000 anchors where 6000 are not cross-boundary\n",
        "    - during testing, clip the proposals that are cross-boundary and return\n",
        "- to reduce redundancy in overlapping proposals, adopt non-max suppression on proposals based on cls scores.\n",
        "    - fix IoU thresh for NMS at 0.7, leaving ultimately about 2000 proposal regions per image\n",
        "    - after NMS, use top-N ranked proposal regions for detection\n",
        "    - Note: can be more lax during training and more strict during testing"
      ],
      "metadata": {
        "id": "HoWK6rIzH44g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiments:\n",
        "- Paper performed experiments on the PASCAL VOC detection benchmark\n",
        "- as well as the MS COCO object detection dataset"
      ],
      "metadata": {
        "id": "zUdpTnZSKCjE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fast R-CNN object detection network"
      ],
      "metadata": {
        "id": "QkaBRcRWRojN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "u-OTcH_z5f7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1VtsHNbdJ-N6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained = tf.keras.applications.vgg16.VGG16(\n",
        "    include_top=False, weights='imagenet', classes=1000\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJsfxk_86yCr",
        "outputId": "23fc4554-2fdb-469f-d777-af13e6243846"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 0s 0us/step\n",
            "58900480/58889256 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSXv4D_S7H08",
        "outputId": "bf6b5fcb-47bd-405d-c2f8-65410a025697"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, None, None, 3)]   0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, None, None, 64)    1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, None, None, 64)    36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, None, None, 64)    0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, None, None, 128)   73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, None, None, 128)   147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, None, None, 128)   0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, None, None, 256)   295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, None, None, 256)   590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, None, None, 256)   590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, None, None, 256)   0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, None, None, 512)   1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, None, None, 512)   2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, None, None, 512)   2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, None, None, 512)   0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, None, None, 512)   2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, None, None, 512)   2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, None, None, 512)   2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, None, None, 512)   0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 14,714,688\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RPN(keras.Model):\n",
        "    def __init__(self, pretrained, n=3, k=9):\n",
        "        super(RPN, self).__init__()\n",
        "        self.pretrained = pretrained\n",
        "\n",
        "        self.windowmap = keras.Conv2D(512, (n,n), activation=\"relu\")\n",
        "        self.reglayer = keras.Conv2D(4*k, (1,1))\n",
        "        self.clslayer = keras.Conv2D(2*k, (1,1), activation=\"sigmoid\")\n",
        "\n",
        "    def call(self, input_images):\n",
        "        x = self.pretrained(input_images)\n",
        "        x = self.windowmap(input_images)\n",
        "        reg = self.reglayer(x)\n",
        "        cls = self.clslayer(x)\n",
        "\n",
        "        "
      ],
      "metadata": {
        "id": "JmXg5_o05nL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Classifier(keras.Model):\n",
        "    def __init__(self, pretrained):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.pretrained = pretrained\n",
        "    \n",
        "    def call(self, input_images):\n",
        "        pass"
      ],
      "metadata": {
        "id": "8zduCAeC968H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FasterRCNN(keras.Model):\n",
        "    def __init__(self, RPN, Classifier):\n",
        "        super(FasterRCNN, self).__init__()\n",
        "        self.RPN = RPN\n",
        "        self.Classifier = Classifier\n",
        "\n",
        "    def compile(self, optimizer):\n",
        "        super(FasterRCNN, self).compile()\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "    def call(self, input_images):\n",
        "        rpn_out = self.RPN(input_images)\n",
        "        classifier_out = self.Classifier(input_images)\n",
        "        return tf.concat([rpn_out, classifier_out])\n",
        "\n",
        "    def train_step(self, input_images):\n",
        "        with tf.GradientTape() as tape:\n",
        "            out = self(input_images)\n",
        "            loss = loss_fn(out, tar)\n",
        "        grads = tape.gradient(loss, self.trainable_weights)\n",
        "        optimizer = self.optimizer.apply_gradients(\n",
        "            zip(grads, self.trainable_weights)\n",
        "        )\n",
        "        return {\"loss\": loss}"
      ],
      "metadata": {
        "id": "VNwzTGq49_Qd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}