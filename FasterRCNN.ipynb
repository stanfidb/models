{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FasterRCNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMptg/gI7z9akC1kI3ui/1O"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Faster RCNN"
      ],
      "metadata": {
        "id": "mRrAVOpLJ_pZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Components\n"
      ],
      "metadata": {
        "id": "Xfzlh6STKBa1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Region Proposal Network (RPN)\n",
        "- input: image of any size\n",
        "- output: set of rectangular object proposals, each w/ an \"objectness\" score\n",
        "- black box: \n",
        "    - n x n spatial window slides across feature map output\n",
        "        - ie. n x n convolutional layer\n",
        "        - w/ n=3, ZF/VGG receptive field size is 171/228 pixels\n",
        "    - window maps to lower-dimensional feature (256-d/512-d w/ ReLU)\n",
        "    - feed feature into 2 sibling FCNs\n",
        "        - Box-regression layer (reg)\n",
        "        - Box-classification layer (cls)\n",
        "        - ie. 1 x 1 convolutional layers\n",
        "    - at each sliding-window location, predict multiple proposals given number of anchors k\n",
        "        - therefore, reg layer has 4k outputs encoding coordinates\n",
        "            - parameterize w.r.t k reference boxes\n",
        "        - therefore, cls layer has 2k outputs estimating probability of object or not object\n",
        "        - each anchor has specified scale and aspect ratio\n",
        "            - use 3 scales, 3 aspect ratios for k = 9 anchors\n",
        "        - translation-invariant anchors:\n",
        "            - regress bounding boxes w/ ref to anchor boxes\n",
        "    - loss function:\n",
        "        - for training, assign binary class label (obj or not) to each anchor\n",
        "            - Positive:\n",
        "                - anchor/anchors w/ highest IoU overlap w/ ground-truth box (rare case)\n",
        "                - OR\n",
        "                - anchor that has IoU overlap higher than 0.7 with any ground-truth box (most common case)\n",
        "            - single ground-truth box may assign positive labels to multiple anchors\n",
        "            - Negative:\n",
        "                - non-positive anchor if its IoU ratio is lower than 0.3 for all ground-truth boxes\n",
        "            - Neither:\n",
        "                - then anchors don't contribute to training\n",
        "        - Loss Function defined in paper\n",
        "            - Lcls is log loss over two classes\n",
        "            - Lreg is robust loss function (ie. smooth L1)\n",
        "            - regression loss activated only for positive anchors\n",
        "            - normalization by scale factor (optional)\n",
        "        - bbox parameterization:\n",
        "            - define w/ log and distance from corners of bbox and size of box\n",
        "            - essentially, regression from anchor box to nearby ground-truth box\n",
        "            - translation-invariant\n",
        "    - Training RPNs\n",
        "        - stochastic gradient descent\n",
        "        - each mini-batch arises from single image containing many positive and negative anchors\n",
        "        - to prevent bias towards the more common negative anchors, randomly sample 256 anchors in image w/ up to 1:1 ratio\n",
        "            - if fewer than 128 positive samples, pad mini-batch w/ negative ones\n",
        "        - randomly initialize all new layers by drawing weights form zero-mean Gauss distrib. w/ std of 0.01\n",
        "            - all other layers (ie. shared conv layers) are initialized by pretraining model for ImageNet classification (ie. fine-tune ZF, VGG, etc.)\n",
        "        - lr = 0.001 for 60k mini-batches\n",
        "        - lr = 0.0001 for next 20k mini-batches\n",
        "        - momentum = 0.9\n",
        "        - weight decay = 0.0005"
      ],
      "metadata": {
        "id": "Bu5YpzeARb05"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training: 4-Step Alternating Training\n",
        "1. fine-tune pre-trained ImageNet model for RPN\n",
        "2. train separate detection network by Fast R-CNN using generated proposals from step-1 (also a pre-trained ImageNet model)\n",
        "3. use detector network to initialize RPN training, but fix the shared conv layers and only fine-tune the layers unique to RPN (now two networks share conv layers)\n",
        "4. keeping shared layers fixed, fine-tune unique layers of Fast R-CNN\n"
      ],
      "metadata": {
        "id": "xB84M5_MaJF-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training: Approximate joint training"
      ],
      "metadata": {
        "id": "cuO7COVYciMr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training: Non-approximate joint training"
      ],
      "metadata": {
        "id": "lmRTKHKJcp9K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fast R-CNN object detection network"
      ],
      "metadata": {
        "id": "QkaBRcRWRojN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "u-OTcH_z5f7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1VtsHNbdJ-N6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained = tf.keras.applications.vgg16.VGG16(\n",
        "    include_top=False, weights='imagenet', classes=1000\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJsfxk_86yCr",
        "outputId": "23fc4554-2fdb-469f-d777-af13e6243846"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 0s 0us/step\n",
            "58900480/58889256 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSXv4D_S7H08",
        "outputId": "bf6b5fcb-47bd-405d-c2f8-65410a025697"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, None, None, 3)]   0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, None, None, 64)    1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, None, None, 64)    36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, None, None, 64)    0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, None, None, 128)   73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, None, None, 128)   147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, None, None, 128)   0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, None, None, 256)   295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, None, None, 256)   590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, None, None, 256)   590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, None, None, 256)   0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, None, None, 512)   1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, None, None, 512)   2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, None, None, 512)   2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, None, None, 512)   0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, None, None, 512)   2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, None, None, 512)   2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, None, None, 512)   2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, None, None, 512)   0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 14,714,688\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RPN(keras.Model):\n",
        "    def __init__(self, pretrained, n=3, k=9):\n",
        "        super(RPN, self).__init__()\n",
        "        self.pretrained = pretrained\n",
        "\n",
        "        self.windowmap = keras.Conv2D(512, (n,n), activation=\"relu\")\n",
        "        self.reglayer = keras.Conv2D(4*k, (1,1))\n",
        "        self.clslayer = keras.Conv2D(2*k, (1,1), activation=\"sigmoid\")\n",
        "\n",
        "    def call(self, input_images):\n",
        "        x = self.pretrained(input_images)\n",
        "        x = self.windowmap(input_images)\n",
        "        reg = self.reglayer(x)\n",
        "        cls = self.clslayer(x)\n",
        "\n",
        "        "
      ],
      "metadata": {
        "id": "JmXg5_o05nL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Classifier(keras.Model):\n",
        "    def __init__(self, pretrained):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.pretrained = pretrained\n",
        "    \n",
        "    def call(self, input_images):\n",
        "        pass"
      ],
      "metadata": {
        "id": "8zduCAeC968H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FasterRCNN(keras.Model):\n",
        "    def __init__(self, RPN, Classifier):\n",
        "        super(FasterRCNN, self).__init__()\n",
        "        self.RPN = RPN\n",
        "        self.Classifier = Classifier\n",
        "\n",
        "    def compile(self, optimizer):\n",
        "        super(FasterRCNN, self).compile()\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "    def call(self, input_images):\n",
        "        rpn_out = self.RPN(input_images)\n",
        "        classifier_out = self.Classifier(input_images)\n",
        "        return tf.concat([rpn_out, classifier_out])\n",
        "\n",
        "    def train_step(self, input_images):\n",
        "        with tf.GradientTape() as tape:\n",
        "            out = self(input_images)\n",
        "            loss = loss_fn(out, tar)\n",
        "        grads = tape.gradient(loss, self.trainable_weights)\n",
        "        optimizer = self.optimizer.apply_gradients(\n",
        "            zip(grads, self.trainable_weights)\n",
        "        )\n",
        "        return {\"loss\": loss}"
      ],
      "metadata": {
        "id": "VNwzTGq49_Qd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}